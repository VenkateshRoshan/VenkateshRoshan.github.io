<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Butchi Venkatesh Adari's Portfolio</title>
    <link rel="stylesheet" href="styles.css">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet"> -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="#">Home</a></li>
                <li><a href="Butchi_Venkatesh_Adari_Resume.pdf">Resume</a></li>
                <li><a href="#cv">CV</a></li>
                <li><a href="https://github.com/VenkateshRoshan" target="_blank">GitHub</a></li>
                <li><a href="mailto:butchivenkatesh.a@gmail.com">Mail</a></li>
                <li>
                    <a href="https://www.linkedin.com/in/abven/" target="_blank" rel="noopener noreferrer">
                        <i class="fab fa-linkedin"></i> LinkedIn
                    </a>
                </li>
            </ul>
        </nav>
    </header>

    <main>
        <section id="home">
            <h1>Butchi Venkatesh Adari</h1>
            <p>A passionate and dedicated Machine Learning and MLOps professional with a strong foundation in AI, computer vision, and robotics. I hold expertise in developing, deploying, and optimizing AI models with hands-on experience in cloud platforms like AWS and Google Cloud. My work spans across various domains, including reinforcement learning, depth estimation, real-time object detection, and conversational AI systems.

                I have worked on several impactful projects, including developing a real-time customer support chatbot deployed on AWS SageMaker, fine-tuning depth estimation models for robotics, and building a simulated vehicle environment for object tracking and lane detection. I am currently focused on implementing advanced AI systems for robotics, including object grasping, reinforcement learning, and AI-driven robotic agents.
                
                My goal is to drive innovation in the AI/ML field, leveraging modern tools and techniques to create practical solutions for real-world challenges.
                
                </p>
        <section id="profile">
            <h2>My Profile</h2>
            <img src="ProfilePicture.jpg" alt="Profile Picture" class="profile-picture">
            <p></p>
        </section>

        <section id="projects">
            <h2>My Projects</h2>
            <!-- Add this project block within your projects section -->
            <div class="project">
                <h3>Enhanced Monocular Depth Estimation with Grasp Pose Prediction</h3>
                <p>Successfully implemented a groundbreaking project in robotic vision and manipulation that combines advanced depth estimation with practical grasping capabilities. 
                Fine-tuned Apple's Depth-Pro model to achieve significant improvements in monocular depth estimation, particularly excelling in scenarios where traditional depth sensors fail. 
                When tested with the ReactorX-200 robotic arm at 30cm height, conventional depth sensors produced null values, while our enhanced monocular depth system maintained accuracy with just 1-2cm error margin.
                Further optimized the model through quantization, achieving comparable performance with FP16 precision while maintaining minimal error rates (note: INT8 quantization showed increased error margins).
                Developed and deployed an innovative Grasp Transformer model that leverages RGB images and monocular depth data to predict optimal grasp poses for robotic manipulation. 
                The system has been successfully deployed on the ReactorX-200 (Trossen Robotics) platform, demonstrating robust performance in real-world grasping tasks.
                    There is an attached video which shows the ReactorX-200 arm successfully grasping a ball from a distance of 15cm, showcasing the practical application of our depth estimation and grasp pose prediction system.</p>
                </p>
                
                <div class="slideshow-container">
                    <div class="slides fade">
                        <img src="Grasping/comparison.png" alt="Depth Estimation Comparison" width="1000" height="600">
                        <div class="caption">Depth Estimation Comparison</div>
                    </div>
                    
                    <div class="slides fade">
                        <img src="Grasping/quantization_comparison.png" alt="Quantization Comparison" width="1000" height="600">
                        <div class="caption">Quantization Comparison</div>
                    </div>

                    <div class="slides fade">
                        <div class="project-video-container">
                            <video 
                                class="project-video"
                                controls
                                preload="metadata"
                                poster="video-thumbnail.jpg">
                                <source src="Grasping/Grasping-rx200.mp4" type="video/mp4">
                                Reactor-X 200 grasping a ball under 15cm view.
                            </video>
                        </div>
                        <div class="caption">Reactor-X 200 grasping a ball under 15cm view.</div>
                    </div>
            
                    <!-- Navigation buttons -->
                    <a class="prev" onclick="changeSlide(-1)">&#10094;</a>
                    <a class="next" onclick="changeSlide(1)">&#10095;</a>
                </div>
                <!-- <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/Research-Paper-RAG" target="_blank">Git Repo</a>
                </div> -->

            </div>
            <div class="project">
                <h3>Rufus - Intelligent Web Data Extractor for RAG Systems</h3>
                
                <p>ðŸ¤– Engineered a cutting-edge AI-powered web extraction system that revolutionizes how data is gathered and processed for RAG (Retrieval-Augmented Generation) applications. Rufus intelligently crawls websites based on natural language instructions, making complex data extraction as simple as writing a prompt.</p>
                
                <div class="project-details">
                    <h4>Key Features</h4>
                    <ul>
                        <li><strong>AI-Driven Crawling:</strong> Implemented advanced crawling algorithms powered by LLMs (TinyLlama, with planned OpenAI integration) that understand and follow natural language instructions for targeted data extraction.</li>
                        <li><strong>Smart Content Selection:</strong> Developed intelligent filtering mechanisms that identify and extract only the most relevant content, significantly reducing noise in the final dataset.</li>
                        <li><strong>Dynamic Navigation:</strong> Built robust handling for complex web structures, including nested links and dynamically loaded content, ensuring comprehensive data collection.</li>
                        <li><strong>Structured Output:</strong> Created sophisticated document synthesis pipelines that convert raw web content into clean, structured formats ready for RAG applications.</li>
                    </ul>
            
                    <h4>Technical Implementation</h4>
                    <ul>
                        <li><strong>Core Architecture:</strong> Built with Python, utilizing FastAPI for REST endpoints and Gradio for an intuitive user interface. Containerized with Docker for consistent deployment across environments.</li>
                        <li><strong>LLM Integration:</strong> Leveraged LangChain for orchestrating LLM operations, initially using Ollama-TinyLlama for development with seamless migration path to OpenAI for production.</li>
                        <li><strong>Vector Storage:</strong> Implemented ChromaDB for efficient storage and retrieval of document embeddings, enabling fast and accurate content matching.</li>
                        <li><strong>Modular Design:</strong> Created a highly maintainable codebase with clear separation of concerns across crawling, AI processing, and data synthesis components.</li>
                    </ul>
            
                    <h4>Performance & Scalability</h4>
                    <ul>
                        <li><strong>Intelligent Rate Limiting:</strong> Implemented adaptive rate limiting and concurrent processing to maximize throughput while respecting website policies.</li>
                        <li><strong>Error Resilience:</strong> Built comprehensive error handling and recovery mechanisms, ensuring reliable operation across diverse web environments.</li>
                        <li><strong>Monitoring & Logging:</strong> Integrated detailed logging and monitoring capabilities for tracking system performance and debugging.</li>
                    </ul>
            
                    <!-- <h4>Example Usage</h4>
                    <pre><code>
            from Rufus import RufusClient
            
            # Initialize client
            client = RufusClient(api_key=YOUR_API_KEY)
            
            # Extract data with natural language instructions
            documents = client.scrape(
                url="https://example.com",
                instructions="Find all product features and customer FAQ sections"
            )
            
            # Documents are returned in structured format ready for RAG pipelines
            print(documents)
                    </code></pre> -->
                </div>
            
                <img src="AIWebExtractor/output_project-4.png" alt="Rufus Project Architecture" class="project-image" width="1000" height="600">
                
                <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/rufus-web-extractor" target="_blank">Git Repo</a>
                </div>
            </div>
            <div class="project">
                <!-- Add project Title -->
                <h3>Research-Paper-RAG</h3>
                <p>Developed a production-ready Research Paper Analysis System leveraging Retrieval-Augmented Generation (RAG) architecture. The system efficiently processes academic papers from the arXiv dataset, utilizing FAISS for similarity search and Flan-T5 for text generation. 
                    Built on Google Cloud Platform, it features a FastAPI backend deployed on Vertex AI, with a user-friendly Gradio interface for real-time querying. The system employs advanced embedding techniques to understand and retrieve relevant research papers, 
                    generating context-aware responses to user queries. Implemented with a robust CI/CD pipeline using GitHub Actions, the system includes comprehensive monitoring, automated deployments, and scalable infrastructure. The architecture ensures efficient handling of 
                    large-scale document processing while maintaining quick response times and high accuracy in research paper analysis.</p>
                <div class="slideshow-container">
                    <div class="slides fade">
                        <img src="Research-Paper-RAG/outputRAGSystem.png" alt="RAG Results" width="1000" height="600">
                        <div class="caption">Research Paper RAG System</div>
                    </div>
                    
                    <div class="slides fade">
                        <img src="Research-Paper-RAG/Architecture.png" alt="RAG System Architecture" width="1000" height="600">
                        <div class="caption">System Architecture</div>
                    </div>
                    
                    <div class="slides fade">
                        <img src="Research-Paper-RAG/originalOutput_flanT5.png" alt="RAG Based Model Output" width="1000" height="600">
                        <div class="caption">Based Model Result</div>
                    </div>
            
                    <!-- Navigation buttons -->
                    <a class="prev" onclick="changeSlide(-1)">&#10094;</a>
                    <a class="next" onclick="changeSlide(1)">&#10095;</a>
                </div>
                <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/Research-Paper-RAG" target="_blank">Git Repo</a>
                </div>
            </div>
            <div class="project">
                <!-- Add project Title -->
                <h3>Image Captioning with Vision Transformer and GPT 2</h3>
                <p>This project is an image caption generator that uses a Vision Transformer (ViT) to extract visual features from images and a GPT-2 model to generate natural language descriptions. 
                    The model is deployed on both AWS and Hugging Face, allowing users to upload images and receive descriptive captions in real-time.</p>
                <img src="ImageCaptionining/output_project-1.png" alt="Project 2" class="project-image" width="1000" height="600">
                <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/Image-Captioning-with-Vision-Transformer-and-GPT-2" target="_blank">Git Repo</a>
                    <a href="https://huggingface.co/spaces/abven/ImageCaptionGenerator" target="_blank">Hugging Face Site</a>
                </div>
            </div>
            <div class="project">
                <h3>Real Time Customer Support Chatbot</h3>
                <p>ðŸ¤– Developed and deployed an enterprise-level customer support chatbot leveraging LLM technology for automated, context-aware responses. Implemented comprehensive MLOps practices with 
                    full monitoring and versioning capabilities. Containerized the solution with Docker, featuring an interactive Gradio UI for seamless model interaction and testing.</p>
                <img src="CustomerChatbot/output_project-2.png" alt="Project 3" class="project-image" width="1000" height="600">
                <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/Real-Time-Customer-Support-Chatbot" target="_blank">Git Repo</a>
                    <a href="https://huggingface.co/spaces/abven/Customer-Support-Chatbot" target="_blank">huggingface Site</a>
                </div>
            </div>

            <div class="project">
                <h3>Miniature Self Driving Car</h3>
                <p>Implemented a high-performance object detection system on Raspberry Pi 4, achieving 98% accuracy in real-time identification of traffic signs, pedestrians, and obstacles. Demonstrated practical application through successful 
                    integration with a miniature autonomous vehicle, enabling self-guided navigation in controlled environments.</p>
            
                <!-- Option 2: For Local Video File -->
                <div class="project-video-container">
                    <video 
                        class="project-video"
                        controls
                        preload="metadata"
                        poster="video-thumbnail.jpg">
                        <source src="SelfDrivingCar/AVM_Output.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            
                <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/Autonomous-Vehicle-Model/tree/master" target="_blank">Git Repo</a>
                </div>
            </div>
            <!-- Add more projects as needed -->
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Butchi Venkatesh Adari</p>
    </footer>

    <script>
        let slideIndex = 1;
        showSlides(slideIndex);
        
        function changeSlide(n) {
            showSlides(slideIndex += n);
        }
        
        function currentSlide(n) {
            showSlides(slideIndex = n);
        }
        
        function showSlides(n) {
            let i;
            let slides = document.getElementsByClassName("slides");
            let dots = document.getElementsByClassName("dot");
            
            if (n > slides.length) {slideIndex = 1}
            if (n < 1) {slideIndex = slides.length}
            
            for (i = 0; i < slides.length; i++) {
                slides[i].style.display = "none";
            }
            for (i = 0; i < dots.length; i++) {
                dots[i].className = dots[i].className.replace(" active", "");
            }
            
            slides[slideIndex-1].style.display = "block";
            dots[slideIndex-1].className += " active";
        }
        
        // Auto-advance slides every 5 seconds
        setInterval(() => {
            changeSlide(1);
        }, 5000);

        // // Smooth scroll for navigation links
        // document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        //     anchor.addEventListener('click', function (e) {
        //         e.preventDefault();
        //         document.querySelector(this.getAttribute('href')).scrollIntoView({
        //             behavior: 'smooth'
        //         });
        //     });
        // });

    </script>
</body>
</html>
