<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Butchi Venkatesh Adari's Portfolio</title>
    <link rel="stylesheet" href="styles.css">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet"> -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <style>
        /* Override display property for the first slide in each slideshow */
        .slideshow-container .slides:first-child {
            display: block !important;
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="#">Home</a></li>
                <li><a href="Butchi_Venkatesh_Adari_Resume.pdf">Resume</a></li>
                <li><a href="#cv">CV</a></li>
                <li><a href="https://github.com/VenkateshRoshan" target="_blank">GitHub</a></li>
                <li><a href="mailto:butchivenkatesh.a@gmail.com">Mail</a></li>
                <li>
                    <a href="https://www.linkedin.com/in/abven/" target="_blank" rel="noopener noreferrer">
                        <i class="fab fa-linkedin"></i> LinkedIn
                    </a>
                </li>
            </ul>
        </nav>
    </header>

    <main>
        <section id="home">
            <h1>Butchi Venkatesh Adari</h1>
        <!-- <section id="profile">
            <h2>My Profile</h2>
            <img src="ProfilePicture.jpg" alt="Profile Picture" class="profile-picture">
            <p></p>
        </section> -->

        <section id="about">
            <h2>About Me</h2>
            <div class="about-me-container">
                <img src="ProfilePicture.jpg" alt="Profile Picture" class="profile-picture-inline">
                <p class="about-me-text">
                    I'm Butchi Venkatesh Adari, a passionate Robotics Engineer and Machine Learning Researcher driven by real-world impact. 
                    With a Master's in Robotics Engineering from WPI and a Bachelor's in Computer Science, I've built a strong foundation 
                    in AI, perception, and automation. My work focuses on intelligent systems from robotic grasping with monocular depth 
                    to LLM-driven data pipelines. I enjoy transforming complex problems into elegant AI-powered solutions, combining vision, 
                    language, and control to push boundaries in robotics and MLOps.
                </p>
            </div>
        </section>

        <section id="experience">
            <h2>Experience</h2>
            <div class="timeline">

                <div class="timeline-item">
                    <div class="timeline-date">2023 â€“ 2025</div>
                    <div class="timeline-content">
                        <h3>Graduate Researcher</h3>
                        <p class="experience-meta">ELPIS Lab, Worcester Polytechnic Institute</p>
                        <p>Conducted cutting-edge research on monocular depth estimation and grasp pose prediction using Apple's Depth Pro and ViT-based Grasp Transformers. Deployed a real-time system on ReactorX-200 for robotic grasping tasks.</p>
                        <img src="Images/wpi_logo.png" alt="WPI Logo" class="experience-logo-timeline">
                    </div>
                </div>


                <div class="timeline-item">
                    <div class="timeline-date">2021 â€“ 2023</div>
                    <div class="timeline-content">
                        <h3>Machine Learning Engineer</h3>
                        <p class="experience-meta">Tata Consultancy Services</p>
                        <p>Built YOLOv5 + DeepSORT pipelines for people tracking and OCR-based entity extraction for enterprise clients. Delivered containerized AI pipelines with FastAPI and deployed on cloud infrastructure.</p>
                        <img src="Images/TCS_logo.webp" alt="TCS Logo" class="experience-logo-timeline">
                    </div>
                </div>


            </div>
        </section>

        <section id="education">
            <h2>Education</h2>
            <div class="timeline education-timeline">

                <div class="timeline-item">
                    <div class="timeline-date">2023 â€“ 2025</div>
                    <div class="timeline-content">
                        <h3>Worcester Polytechnic Institute (WPI)</h3>
                        <p class="experience-meta">M.S. in Robotics Engineering</p>
                        <p>Focused on robotic perception, control systems, and AI-driven automation. Worked on real-world grasping systems, LLM integration, and computer vision pipelines for robotic platforms.</p>
                        <img src="Images/wpi_logo.png" alt="WPI Logo" class="edu-logo-timeline">
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-date">2017 â€“ 2021</div>
                    <div class="timeline-content">
                        <h3>Anil Neerukonda Institute of Technology & Sciences (ANITS)</h3>
                        <p class="experience-meta">B.Tech in Computer Science and Engineering</p>
                        <p>Gained strong fundamentals in algorithms, software development, and systems engineering. Developed a passion for machine learning and intelligent systems.</p>
                        <img src="Images/Anits_logo.jpg" alt="ANITS Logo" class="edu-logo-timeline">
                    </div>
                </div>

            </div>
        </section>

        <section id="projects">
            <h2>My Projects</h2>
            <!-- Add this project block within your projects section -->
            <div class="project">
                <h3>Enhanced Monocular Depth Estimation with Grasp Pose Prediction</h3>
                <p>Successfully implemented a groundbreaking project in robotic vision and manipulation that combines advanced depth estimation with practical grasping capabilities. 
                Fine-tuned Apple's Depth-Pro model to achieve significant improvements in monocular depth estimation, particularly excelling in scenarios where traditional depth sensors fail. 
                When tested with the ReactorX-200 robotic arm at 30cm height, conventional depth sensors produced null values, while our enhanced monocular depth system maintained accuracy with just 1-2cm error margin.
                Further optimized the model through quantization, achieving comparable performance with FP16 precision while maintaining minimal error rates (note: INT8 quantization showed increased error margins).
                Developed and deployed an innovative Grasp Transformer model that leverages RGB images and monocular depth data to predict optimal grasp poses for robotic manipulation. 
                The system has been successfully deployed on the ReactorX-200 (Trossen Robotics) platform, demonstrating robust performance in real-world grasping tasks.
                    There is an attached video which shows the ReactorX-200 arm successfully grasping a ball from a distance of 15cm, showcasing the practical application of our depth estimation and grasp pose prediction system.</p>
                </p>
                
                <div class="slideshow-container">
                    <div class="slides fade">
                        <img src="Grasping/comparison.png" alt="Depth Estimation Comparison" width="1000" height="600">
                        <div class="caption">Depth Estimation Comparison</div>
                    </div>
                    
                    <div class="slides fade">
                        <img src="Grasping/quantization_comparison.png" alt="Quantization Comparison" width="1000" height="600">
                        <div class="caption">Quantization Comparison</div>
                    </div>

                    <div class="slides fade">
                        <div class="project-video-container">
                            <video 
                                class="project-video"
                                controls
                                preload="metadata"
                                poster="video-thumbnail.jpg">
                                <source src="Grasping/Grasping-rx200.mp4" type="video/mp4">
                                Reactor-X 200 grasping a ball under 15cm view.
                            </video>
                        </div>
                        <div class="caption">Reactor-X 200 grasping a ball under 15cm view.</div>
                    </div>
            
                    <!-- Navigation buttons -->
                    <a class="prev" onclick="changeSlide(-1, this)">&#10094;</a>
                    <a class="next" onclick="changeSlide(1, this)">&#10095;</a>
                </div>
                <!-- <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/Research-Paper-RAG" target="_blank">Git Repo</a>
                </div> -->

            </div>
            <div class="project">
                <h3>Rufus - Intelligent Web Data Extractor for RAG Systems</h3>
                
                <p>ðŸ¤– Engineered a cutting-edge AI-powered web extraction system that revolutionizes how data is gathered and processed for RAG (Retrieval-Augmented Generation) applications. Rufus intelligently crawls websites based on natural language instructions, making complex data extraction as simple as writing a prompt.</p>
                
                <div class="project-details">
                    <h4>Key Features</h4>
                    <ul>
                        <li><strong>AI-Driven Crawling:</strong> Implemented advanced crawling algorithms powered by LLMs (TinyLlama, with planned OpenAI integration) that understand and follow natural language instructions for targeted data extraction.</li>
                        <li><strong>Smart Content Selection:</strong> Developed intelligent filtering mechanisms that identify and extract only the most relevant content, significantly reducing noise in the final dataset.</li>
                        <li><strong>Dynamic Navigation:</strong> Built robust handling for complex web structures, including nested links and dynamically loaded content, ensuring comprehensive data collection.</li>
                        <li><strong>Structured Output:</strong> Created sophisticated document synthesis pipelines that convert raw web content into clean, structured formats ready for RAG applications.</li>
                    </ul>
            
                    <h4>Technical Implementation</h4>
                    <ul>
                        <li><strong>Core Architecture:</strong> Built with Python, utilizing FastAPI for REST endpoints and Gradio for an intuitive user interface. Containerized with Docker for consistent deployment across environments.</li>
                        <li><strong>LLM Integration:</strong> Leveraged LangChain for orchestrating LLM operations, initially using Ollama-TinyLlama for development with seamless migration path to OpenAI for production.</li>
                        <li><strong>Vector Storage:</strong> Implemented ChromaDB for efficient storage and retrieval of document embeddings, enabling fast and accurate content matching.</li>
                        <li><strong>Modular Design:</strong> Created a highly maintainable codebase with clear separation of concerns across crawling, AI processing, and data synthesis components.</li>
                    </ul>
            
                    <h4>Performance & Scalability</h4>
                    <ul>
                        <li><strong>Intelligent Rate Limiting:</strong> Implemented adaptive rate limiting and concurrent processing to maximize throughput while respecting website policies.</li>
                        <li><strong>Error Resilience:</strong> Built comprehensive error handling and recovery mechanisms, ensuring reliable operation across diverse web environments.</li>
                        <li><strong>Monitoring & Logging:</strong> Integrated detailed logging and monitoring capabilities for tracking system performance and debugging.</li>
                    </ul>
            
                    <!-- <h4>Example Usage</h4>
                    <pre><code>
            from Rufus import RufusClient
            
            # Initialize client
            client = RufusClient(api_key=YOUR_API_KEY)
            
            # Extract data with natural language instructions
            documents = client.scrape(
                url="https://example.com",
                instructions="Find all product features and customer FAQ sections"
            )
            
            # Documents are returned in structured format ready for RAG pipelines
            print(documents)
                    </code></pre> -->
                </div>
            
                <img src="AIWebExtractor/output_project-4.png" alt="Rufus Project Architecture" class="project-image" width="1000" height="600">
                
                <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/rufus-web-extractor" target="_blank">Git Repo</a>
                </div>
            </div>
            <div class="project">
                <!-- Add project Title -->
                <h3>Research-Paper-RAG</h3>
                <p>Developed a production-ready Research Paper Analysis System leveraging Retrieval-Augmented Generation (RAG) architecture. The system efficiently processes academic papers from the arXiv dataset, utilizing FAISS for similarity search and Flan-T5 for text generation. 
                    Built on Google Cloud Platform, it features a FastAPI backend deployed on Vertex AI, with a user-friendly Gradio interface for real-time querying. The system employs advanced embedding techniques to understand and retrieve relevant research papers, 
                    generating context-aware responses to user queries. Implemented with a robust CI/CD pipeline using GitHub Actions, the system includes comprehensive monitoring, automated deployments, and scalable infrastructure. The architecture ensures efficient handling of 
                    large-scale document processing while maintaining quick response times and high accuracy in research paper analysis.</p>
                <div class="slideshow-container">
                    <div class="slides fade">
                        <img src="Research-Paper-RAG/outputRAGSystem.png" alt="RAG Results" width="1000" height="600">
                        <div class="caption">Research Paper RAG System</div>
                    </div>
                    
                    <div class="slides fade">
                        <img src="Research-Paper-RAG/Architecture.png" alt="RAG System Architecture" width="1000" height="600">
                        <div class="caption">System Architecture</div>
                    </div>
                    
                    <div class="slides fade">
                        <img src="Research-Paper-RAG/originalOutput_flanT5.png" alt="RAG Based Model Output" width="1000" height="600">
                        <div class="caption">Based Model Result</div>
                    </div>
            
                    <!-- Navigation buttons -->
                    <a class="prev" onclick="changeSlide(-1, this)">&#10094;</a>
                    <a class="next" onclick="changeSlide(1, this)">&#10095;</a>
                </div>
                <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/Research-Paper-RAG" target="_blank">Git Repo</a>
                </div>
            </div>
            <div class="project">
                <!-- Add project Title -->
                <h3>Image Captioning with Vision Transformer and GPT 2</h3>
                <p>This project is an image caption generator that uses a Vision Transformer (ViT) to extract visual features from images and a GPT-2 model to generate natural language descriptions. 
                    The model is deployed on both AWS and Hugging Face, allowing users to upload images and receive descriptive captions in real-time.</p>
                <img src="ImageCaptionining/output_project-1.png" alt="Project 2" class="project-image" width="1000" height="600">
                <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/Image-Captioning-with-Vision-Transformer-and-GPT-2" target="_blank">Git Repo</a>
                    <a href="https://huggingface.co/spaces/abven/ImageCaptionGenerator" target="_blank">Hugging Face Site</a>
                </div>
            </div>
            <div class="project">
                <h3>Real Time Customer Support Chatbot</h3>
                <p>ðŸ¤– Developed and deployed an enterprise-level customer support chatbot leveraging LLM technology for automated, context-aware responses. Implemented comprehensive MLOps practices with 
                    full monitoring and versioning capabilities. Containerized the solution with Docker, featuring an interactive Gradio UI for seamless model interaction and testing.</p>
                <img src="CustomerChatbot/output_project-2.png" alt="Project 3" class="project-image" width="1000" height="600">
                <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/Real-Time-Customer-Support-Chatbot" target="_blank">Git Repo</a>
                    <a href="https://huggingface.co/spaces/abven/Customer-Support-Chatbot" target="_blank">huggingface Site</a>
                </div>
            </div>

            <div class="project">
                <h3>Miniature Self Driving Car</h3>
                <p>Implemented a high-performance object detection system on Raspberry Pi 4, achieving 98% accuracy in real-time identification of traffic signs, pedestrians, and obstacles. Demonstrated practical application through successful 
                    integration with a miniature autonomous vehicle, enabling self-guided navigation in controlled environments.</p>
            
                <!-- Option 2: For Local Video File -->
                <div class="project-video-container">
                    <video 
                        class="project-video"
                        controls
                        preload="metadata"
                        poster="video-thumbnail.jpg">
                        <source src="SelfDrivingCar/AVM_Output.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            
                <div class="project-links">
                    <a href="https://github.com/VenkateshRoshan/Autonomous-Vehicle-Model/tree/master" target="_blank">Git Repo</a>
                </div>
            </div>
            <!-- Add more projects as needed -->
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Butchi Venkatesh Adari</p>
    </footer>

    <script>
        // Initialize all slideshows when the page loads
        document.addEventListener('DOMContentLoaded', function() {
            // Store slide index for each slideshow container
            const slideshows = document.querySelectorAll('.slideshow-container');
            
            // Initialize each slideshow with slide index 1
            slideshows.forEach(function(slideshow) {
                slideshow.setAttribute('data-slide-index', '1');
            });
        });

        // Function to change slides - note the 'buttonElement' parameter
        function changeSlide(n, buttonElement) {
            // Find the parent slideshow container of the clicked button
            const slideshowContainer = buttonElement.closest('.slideshow-container');
            
            // Get current slide index from the container's data attribute
            let slideIndex = parseInt(slideshowContainer.getAttribute('data-slide-index'));
            
            // Update the slide index
            slideIndex += n;
            
            // Get all slides within this specific slideshow container
            const slides = slideshowContainer.querySelectorAll('.slides');
            
            // Handle edge cases
            if (slideIndex > slides.length) {
                slideIndex = 1;
            } else if (slideIndex < 1) {
                slideIndex = slides.length;
            }
            
            // Update the slide index in the container's data attribute
            slideshowContainer.setAttribute('data-slide-index', slideIndex);
            
            // Hide all slides in this slideshow
            slides.forEach(slide => {
                slide.style.display = "none";
            });
            
            // Show the current slide
            if (slides.length > 0) {
                slides[slideIndex - 1].style.display = "block";
            }
        }

        // Auto-advance all slideshows
        setInterval(function() {
            const slideshows = document.querySelectorAll('.slideshow-container');
            
            slideshows.forEach(function(slideshow) {
                // Get the next button for this slideshow
                const nextButton = slideshow.querySelector('.next');
                
                // Simulate a click on the next button if it exists
                if (nextButton) {
                    // Call changeSlide with 1 and pass the next button element
                    changeSlide(1, nextButton);
                }
            });
        }, 5000);
    </script>
</body>
</html>